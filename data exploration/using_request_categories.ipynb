{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.pipeline import Pipeline as imPipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42658/836763382.py:14: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    }
   ],
   "source": [
    "conn = psycopg2.connect(\n",
    "    dbname=\"postgres\", \n",
    "    user=\"root\", \n",
    "    password=\"password1\", \n",
    "    host=\"localhost\", \n",
    "    port=\"5432\"\n",
    ")\n",
    "\n",
    "# Define your SQL query\n",
    "query = 'SELECT * FROM \"Issue\"'\n",
    "\n",
    "\n",
    "# Execute the query and load the result into a DataFrame\n",
    "df = pd.read_sql(query, conn)\n",
    "column_mapping = {\n",
    "    \"Summary\": \"summary\",\n",
    "    \"Issue key\": \"issueKey\",\n",
    "    \"Issue id\": \"issueId\",\n",
    "    \"Issue Type\": \"issueType\",\n",
    "    \"Status\": \"status\",\n",
    "    \"Project key\": \"projectKey\",\n",
    "    \"Project name\": \"projectName\",\n",
    "    \"Priority\": \"priority\",\n",
    "    \"Resolution\": \"resolution\",\n",
    "    \"Assignee\": \"assignee\",\n",
    "    \"Reporter (Email)\": \"reporterEmail\",\n",
    "    \"Creator (Email)\": \"creatorEmail\",\n",
    "    \"Created\": \"created\",\n",
    "    \"Updated\": \"updated\",\n",
    "    \"Last Viewed\": \"lastViewed\",\n",
    "    \"Resolved\": \"resolved\",\n",
    "    \"Due date\": \"dueDate\",\n",
    "    \"Description\": \"description\",\n",
    "    \"Partner Names\": \"partnerNames\",\n",
    "    \"Custom field (Cause of issue)\": \"causeOfIssue\",\n",
    "    \"Custom field (Record/Transaction ID)\": \"recordTransactionId\",\n",
    "    \"Custom field (Region)\": \"region\",\n",
    "    \"Custom field (Relevant Departments)\": \"relevantDepartments\",\n",
    "    \"Custom field (Relevant Departments).1\": \"relevantDepartments1\",\n",
    "    \"Custom field (Request Category)\": \"requestCategory\",\n",
    "    \"Custom field (Request Type)\": \"requestType\",\n",
    "    \"Custom field (Request language)\": \"requestLanguage\",\n",
    "    \"Custom field (Resolution Action)\": \"resolutionAction\",\n",
    "    \"Satisfaction rating\": \"satisfactionRating\",\n",
    "    \"Custom field (Satisfaction date)\": \"satisfactionDate\",\n",
    "    \"Custom field (Source)\": \"source\",\n",
    "    \"Custom field (Time to first response)\": \"timeToFirstResponse\",\n",
    "    \"Custom field (Time to resolution)\": \"timeToResolution\",\n",
    "    \"Custom field (Work category)\": \"workCategory\",\n",
    "    \"Status Category\": \"statusCategory\",\n",
    "    \"Status Category Changed\": \"statusCategoryChanged\",\n",
    "    \"Custom field ([CHART] Date of First Response)\": \"dateOfFirstResponse\",\n",
    "    \"comments\": \"comments\"\n",
    "}\n",
    "\n",
    "# Rename the columns in the DataFrame\n",
    "df.rename(columns=column_mapping, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "825\n"
     ]
    }
   ],
   "source": [
    "df['initial_category'] = df['requestCategory'].str.split('->').str[0].str.strip()\n",
    "df.dropna(subset=[\"requestCategory\", \"initial_category\", \"summary\", \"description\", \"requestType\", \"source\", \"causeOfIssue\"], inplace=True)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Specifying the columns using strings is only supported for pandas DataFrames",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/__init__.py:424\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 424\u001b[0m     all_columns \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/sparse/_base.py:771\u001b[0m, in \u001b[0;36mspmatrix.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 771\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(attr \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: columns not found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 39\u001b[0m\n\u001b[1;32m     33\u001b[0m model \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[1;32m     34\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m\"\u001b[39m, preprocessor),  \u001b[38;5;66;03m# Make sure preprocessing is applied at prediction time\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m, RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m))\n\u001b[1;32m     36\u001b[0m ])\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Train Model on the resampled data\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_resampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_resampled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Predictions on transformed test data (ensure that test data is preprocessed in the same way as training data)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_transformed)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/pipeline.py:401\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \n\u001b[1;32m    377\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    400\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m--> 401\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/pipeline.py:359\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    357\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_steps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/memory.py:312\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/pipeline.py:893\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 893\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    895\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py:724\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_transformers()\n\u001b[0;32m--> 724\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_column_callables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_remainder(X)\n\u001b[1;32m    727\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_transform(X, y, _fit_transform_one)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py:426\u001b[0m, in \u001b[0;36mColumnTransformer._validate_column_callables\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    424\u001b[0m         columns \u001b[38;5;241m=\u001b[39m columns(X)\n\u001b[1;32m    425\u001b[0m     all_columns\u001b[38;5;241m.\u001b[39mappend(columns)\n\u001b[0;32m--> 426\u001b[0m     transformer_to_input_indices[name] \u001b[38;5;241m=\u001b[39m \u001b[43m_get_column_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_columns \u001b[38;5;241m=\u001b[39m all_columns\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer_to_input_indices \u001b[38;5;241m=\u001b[39m transformer_to_input_indices\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/__init__.py:426\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    424\u001b[0m     all_columns \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpecifying the columns using strings is only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported for pandas DataFrames\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m     )\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    431\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [key]\n",
      "\u001b[0;31mValueError\u001b[0m: Specifying the columns using strings is only supported for pandas DataFrames"
     ]
    }
   ],
   "source": [
    "\n",
    "X = df[[\"summary\", \"description\", \"requestType\", \"source\", \"causeOfIssue\"]]\n",
    "y = df[\"initial_category\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing for text features and categorical features\n",
    "text_transformer = TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 2), max_features=5000)\n",
    "cat_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "# Column transformer to apply preprocessing\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"text_summary\", text_transformer, \"summary\"),\n",
    "    (\"text_description\", text_transformer, \"description\"),\n",
    "    (\"cat_features\", cat_transformer, [\"requestType\", \"source\", \"causeOfIssue\"])\n",
    "])\n",
    "\n",
    "# Apply transformations to training data\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "# Apply class balancing using SMOTE with filtering\n",
    "class_counts = Counter(y_train)\n",
    "min_samples = 6\n",
    "\n",
    "# Find the minimum number of samples in any class\n",
    "y_train_filtered = y_train[y_train.isin([k for k, v in class_counts.items() if v >= min_samples])]\n",
    "X_train_filtered = X_train_transformed[y_train.isin([k for k, v in class_counts.items() if v >= min_samples])]\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(sampling_strategy=\"auto\", k_neighbors=min(min_samples - 1, 5), random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train_filtered, y_train_filtered)\n",
    "\n",
    "# Model Pipeline with Preprocessing and Classifier\n",
    "model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),  # Make sure preprocessing is applied at prediction time\n",
    "    (\"classifier\", RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Train Model on the resampled data\n",
    "model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Predictions on transformed test data (ensure that test data is preprocessed in the same way as training data)\n",
    "y_pred = model.predict(X_test_transformed)\n",
    "\n",
    "# Evaluation\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible Model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               precision    recall  f1-score   support\n",
      "\n",
      "        Agency Administration       0.80      0.33      0.47        12\n",
      "             Billing / Grants       0.75      0.60      0.67         5\n",
      "            Delivery / Pickup       0.67      0.50      0.57        24\n",
      "Feedback - Concern / Negative       0.00      0.00      0.00         4\n",
      "                         Menu       1.00      0.50      0.67        10\n",
      "        Orders - Pre delivery       0.75      0.94      0.84       100\n",
      "        Partner Produce Limit       0.83      1.00      0.91         5\n",
      "                  PartnerLink       0.00      0.00      0.00         4\n",
      "             Spam / Duplicate       0.00      0.00      0.00         1\n",
      "\n",
      "                     accuracy                           0.75       165\n",
      "                    macro avg       0.53      0.43      0.46       165\n",
      "                 weighted avg       0.72      0.75      0.71       165\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parker/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/parker/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/parker/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = df[[\"summary\", \"description\", \"requestType\", \"source\", \"causeOfIssue\"]]\n",
    "y = df[\"initial_category\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# # Apply SMOTE to balance the classes\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Preprocessing\n",
    "text_features = [\"summary\", \"description\"]\n",
    "categorical_features = [\"requestType\", \"source\", \"causeOfIssue\"]\n",
    "\n",
    "text_transformer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
    "cat_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"text\", text_transformer, \"summary\"),\n",
    "    (\"text2\", text_transformer, \"description\"),\n",
    "    (\"cat\", cat_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# Model Pipeline\n",
    "model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Train Model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With SMOTE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Specifying the columns using strings is only supported for pandas DataFrames",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/__init__.py:424\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 424\u001b[0m     all_columns \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/sparse/_base.py:771\u001b[0m, in \u001b[0;36mspmatrix.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 771\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(attr \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: columns not found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 39\u001b[0m\n\u001b[1;32m     33\u001b[0m model \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[1;32m     34\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m\"\u001b[39m, preprocessor),  \u001b[38;5;66;03m# Make sure preprocessing is applied at prediction time\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m, RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m))\n\u001b[1;32m     36\u001b[0m ])\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Train Model on the resampled data\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_resampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_resampled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Predictions on transformed test data (ensure that test data is preprocessed in the same way as training data)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_transformed)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/pipeline.py:401\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \n\u001b[1;32m    377\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    400\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m--> 401\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/pipeline.py:359\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    357\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_steps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/memory.py:312\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/pipeline.py:893\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 893\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    895\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py:724\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_transformers()\n\u001b[0;32m--> 724\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_column_callables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_remainder(X)\n\u001b[1;32m    727\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_transform(X, y, _fit_transform_one)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py:426\u001b[0m, in \u001b[0;36mColumnTransformer._validate_column_callables\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    424\u001b[0m         columns \u001b[38;5;241m=\u001b[39m columns(X)\n\u001b[1;32m    425\u001b[0m     all_columns\u001b[38;5;241m.\u001b[39mappend(columns)\n\u001b[0;32m--> 426\u001b[0m     transformer_to_input_indices[name] \u001b[38;5;241m=\u001b[39m \u001b[43m_get_column_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_columns \u001b[38;5;241m=\u001b[39m all_columns\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer_to_input_indices \u001b[38;5;241m=\u001b[39m transformer_to_input_indices\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/__init__.py:426\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    424\u001b[0m     all_columns \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpecifying the columns using strings is only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported for pandas DataFrames\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m     )\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    431\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [key]\n",
      "\u001b[0;31mValueError\u001b[0m: Specifying the columns using strings is only supported for pandas DataFrames"
     ]
    }
   ],
   "source": [
    "\n",
    "X = df[[\"summary\", \"description\", \"requestType\", \"source\", \"causeOfIssue\"]]\n",
    "y = df[\"initial_category\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing for text features and categorical features\n",
    "text_transformer = TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 2), max_features=5000)\n",
    "cat_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "# Column transformer to apply preprocessing\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"text_summary\", text_transformer, \"summary\"),\n",
    "    (\"text_description\", text_transformer, \"description\"),\n",
    "    (\"cat_features\", cat_transformer, [\"requestType\", \"source\", \"causeOfIssue\"])\n",
    "])\n",
    "\n",
    "# Apply transformations to training data\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "# Apply class balancing using SMOTE with filtering\n",
    "class_counts = Counter(y_train)\n",
    "min_samples = 6\n",
    "\n",
    "# Find the minimum number of samples in any class\n",
    "y_train_filtered = y_train[y_train.isin([k for k, v in class_counts.items() if v >= min_samples])]\n",
    "X_train_filtered = X_train_transformed[y_train.isin([k for k, v in class_counts.items() if v >= min_samples])]\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(sampling_strategy=\"auto\", k_neighbors=min(min_samples - 1, 5), random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train_filtered, y_train_filtered)\n",
    "\n",
    "# Model Pipeline with Preprocessing and Classifier\n",
    "model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),  # Make sure preprocessing is applied at prediction time\n",
    "    (\"classifier\", RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Train Model on the resampled data\n",
    "model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Predictions on transformed test data (ensure that test data is preprocessed in the same way as training data)\n",
    "y_pred = model.predict(X_test_transformed)\n",
    "\n",
    "# Evaluation\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost Model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:11:51] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               precision    recall  f1-score   support\n",
      "\n",
      "        Agency Administration       0.50      0.50      0.50        12\n",
      "             Billing / Grants       0.50      0.20      0.29         5\n",
      "            Delivery / Pickup       0.65      0.62      0.64        24\n",
      "Feedback - Concern / Negative       0.00      0.00      0.00         4\n",
      "                         Menu       0.80      0.80      0.80        10\n",
      "        Orders - Pre delivery       0.87      0.95      0.91       100\n",
      "        Partner Produce Limit       0.83      1.00      0.91         5\n",
      "                  PartnerLink       0.50      0.25      0.33         4\n",
      "             Spam / Duplicate       1.00      1.00      1.00         1\n",
      "\n",
      "                     accuracy                           0.80       165\n",
      "                    macro avg       0.63      0.59      0.60       165\n",
      "                 weighted avg       0.77      0.80      0.78       165\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parker/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/parker/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/parker/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df[\"initial_category\"])\n",
    "\n",
    "# Split data into train and test sets\n",
    "X = df[[\"summary\", \"description\", \"requestType\", \"source\", \"causeOfIssue\"]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert categorical variables to one-hot encoding\n",
    "cat_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "class_counts = Counter(y_train)\n",
    "total_samples = len(y_train)\n",
    "class_weights = {cls: total_samples / count for cls, count in class_counts.items()}\n",
    "\n",
    "# Set up the text transformer (TfidfVectorizer)\n",
    "text_transformer = TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 2), max_features=5000)\n",
    "\n",
    "# Preprocessor with text and categorical features\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"text\", text_transformer, \"summary\"),\n",
    "    (\"text2\", text_transformer, \"description\"),\n",
    "    (\"cat\", cat_transformer, [\"requestType\", \"source\", \"causeOfIssue\"])\n",
    "])\n",
    "\n",
    "# XGBoost Pipeline\n",
    "model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", XGBClassifier(scale_pos_weight=list(class_weights.values()), eval_metric=\"mlogloss\"))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "unique_classes = sorted(set(y_test) | set(y_pred))\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_[unique_classes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost wit SMOTE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution: Counter({'Orders - Pre delivery': 368, 'Delivery / Pickup': 86, 'Menu': 49, 'Agency Administration': 45, 'Partner Produce Limit': 35, 'PartnerLink': 29, 'Billing / Grants': 28, 'Feedback - Concern / Negative': 14, 'Spam / Duplicate': 4, 'Feedback - Positive': 1, 'Return Notification': 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:20:44] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               precision    recall  f1-score   support\n",
      "\n",
      "        Agency Administration       0.55      0.50      0.52        12\n",
      "             Billing / Grants       0.60      0.60      0.60         5\n",
      "            Delivery / Pickup       0.58      0.58      0.58        24\n",
      "Feedback - Concern / Negative       0.00      0.00      0.00         4\n",
      "                         Menu       0.70      0.70      0.70        10\n",
      "        Orders - Pre delivery       0.88      0.92      0.90       100\n",
      "        Partner Produce Limit       0.83      1.00      0.91         5\n",
      "                  PartnerLink       0.33      0.25      0.29         4\n",
      "\n",
      "                     accuracy                           0.78       164\n",
      "                    macro avg       0.56      0.57      0.56       164\n",
      "                 weighted avg       0.76      0.78      0.77       164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = df[[\"summary\", \"description\", \"requestType\", \"source\", \"causeOfIssue\"]]\n",
    "y = df[\"initial_category\"]\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert categorical variables to one-hot encoding\n",
    "cat_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "text_transformer = TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 2), max_features=5000)\n",
    "\n",
    "# Check class distribution in the training data\n",
    "class_counts = Counter(y_train)\n",
    "print(f\"Class distribution: {class_counts}\")\n",
    "\n",
    "# Handle classes with very few samples\n",
    "min_samples_per_class = 6  # Adjust this threshold if needed\n",
    "\n",
    "# Filter out classes with fewer than min_samples_per_class samples in y_train\n",
    "valid_classes = [cls for cls, count in class_counts.items() if count >= min_samples_per_class]\n",
    "valid_train_mask = y_train.isin(valid_classes)\n",
    "\n",
    "# Apply the filter to both X_train and y_train\n",
    "X_train = X_train[valid_train_mask]\n",
    "y_train = y_train[valid_train_mask]\n",
    "\n",
    "# Filter the same classes from y_test (optional)\n",
    "valid_test_mask = y_test.isin(valid_classes)\n",
    "X_test = X_test[valid_test_mask]\n",
    "y_test = y_test[valid_test_mask]\n",
    "\n",
    "# SMOTE to balance the classes\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "# Preprocessor for the text and categorical data\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"text\", text_transformer, \"summary\"),\n",
    "    (\"text2\", text_transformer, \"description\"),\n",
    "    (\"cat\", cat_transformer, [\"requestType\", \"source\", \"causeOfIssue\"])\n",
    "])\n",
    "\n",
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Re-fit the label encoder on the filtered y_train\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)  # This should work now\n",
    "\n",
    "# Class balancing and pipeline\n",
    "class_counts = Counter(y_train_encoded)\n",
    "total_samples = len(y_train_encoded)\n",
    "class_weights = {cls: total_samples / count for cls, count in class_counts.items()}\n",
    "\n",
    "# XGBoost model with class weights\n",
    "xgb_model = XGBClassifier(scale_pos_weight=list(class_weights.values()), eval_metric=\"mlogloss\")\n",
    "\n",
    "# Create a pipeline with SMOTE\n",
    "model = imPipeline([\n",
    "    (\"preprocessor\", preprocessor),  # Apply preprocessing steps first\n",
    "    (\"smote\", smote),  # Then apply SMOTE to balance the classes\n",
    "    (\"classifier\", xgb_model)\n",
    "])\n",
    "\n",
    "# Train Model\n",
    "model.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test_encoded, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:54:27] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:54:27] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:54:27] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:54:27] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:54:27] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:54:27] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:54:27] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:54:27] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:54:27] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:54:27] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:54:27] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:54:27] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:54:27] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:54:27] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:54:27] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:54:27] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:54:57] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:54:59] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:54:59] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:55:00] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:55:02] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:55:25] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:55:31] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:55:34] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:55:35] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:55:36] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:55:39] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:55:57] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:55:57] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:56:05] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:56:13] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:56:48] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:56:50] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [11:56:51] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 28\u001b[0m\n\u001b[1;32m     18\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[1;32m     19\u001b[0m     model,\n\u001b[1;32m     20\u001b[0m     param_grid,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Use all available CPU cores for the grid search\u001b[39;00m\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Train the grid search model\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_encoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Best model and hyperparameters\u001b[39;00m\n\u001b[1;32m     31\u001b[0m best_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1387\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    818\u001b[0m         )\n\u001b[1;32m    819\u001b[0m     )\n\u001b[0;32m--> 821\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    # 'classifier__n_jobs': [-1],  # Use all CPU cores\n",
    "    'classifier__max_depth': [3, 5, 7],  # Test different tree depths\n",
    "    'classifier__learning_rate': [0.01, 0.1, 0.3],  # Test different learning rates\n",
    "    'classifier__n_estimators': [50, 100, 200],  # Test different numbers of trees\n",
    "    # 'classifier__subsample': [0.8, 1.0],  # Test subsampling\n",
    "    # 'classifier__colsample_bytree': [0.8, 1.0],  # Test column sampling\n",
    "    # 'classifier__scale_pos_weight': [1, 2, 3],  # Test scale pos weight values\n",
    "}\n",
    "\n",
    "model = imPipeline([\n",
    "    (\"preprocessor\", preprocessor),  # Apply preprocessing steps first\n",
    "    (\"smote\", smote),  # Then apply SMOTE to balance the classes\n",
    "    (\"classifier\", xgb_model)  # Classifier\n",
    "])\n",
    "\n",
    "# Set up the GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    model,\n",
    "    param_grid,\n",
    "    scoring='accuracy',  # You can also use 'f1_macro', 'precision', etc.\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=1,  # Show progress\n",
    "    n_jobs=-1  # Use all available CPU cores for the grid search\n",
    ")\n",
    "\n",
    "# Train the grid search model\n",
    "grid_search.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Best model and hyperparameters\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Predictions from the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test_encoded, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert model: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Convert text to embeddings\n",
    "X_train_text = [summary + \" \" + description for summary, description in zip(X_train[\"summary\"], X_train[\"description\"])]\n",
    "X_test_text = [summary + \" \" + description for summary, description in zip(X_test[\"summary\"], X_test[\"description\"])]\n",
    "\n",
    "X_train_embeddings = bert_model.encode(X_train_text)\n",
    "X_test_embeddings = bert_model.encode(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 660 and the array at index 1 has size 654",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 11\u001b[0m\n\u001b[1;32m      5\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m ColumnTransformer([\n\u001b[1;32m      6\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m\"\u001b[39m, cat_transformer, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequestType\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcauseOfIssue\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      7\u001b[0m ])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Step 3: Combine embeddings with other features\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# If you want to keep the other features, combine them with the embeddings\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m X_train_combined \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_train_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m X_test_combined \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack([X_test_embeddings, preprocessor\u001b[38;5;241m.\u001b[39mtransform(X_test)\u001b[38;5;241m.\u001b[39mtoarray()])\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Step 4: Train an XGBoost model\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mhstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/shape_base.py:370\u001b[0m, in \u001b[0;36mhstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, casting\u001b[38;5;241m=\u001b[39mcasting)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 660 and the array at index 1 has size 654"
     ]
    }
   ],
   "source": [
    "cat_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "text_transformer = TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 2), max_features=5000)\n",
    "\n",
    "# Handle other features (assuming categorical columns are [\"requestType\", \"source\", \"causeOfIssue\"])\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"cat\", cat_transformer, [\"requestType\", \"source\", \"causeOfIssue\"])\n",
    "])\n",
    "\n",
    "# Step 3: Combine embeddings with other features\n",
    "# If you want to keep the other features, combine them with the embeddings\n",
    "X_train_combined = np.hstack([X_train_embeddings, preprocessor.fit_transform(X_train).toarray()])\n",
    "X_test_combined = np.hstack([X_test_embeddings, preprocessor.transform(X_test).toarray()])\n",
    "\n",
    "# Step 4: Train an XGBoost model\n",
    "xgb_model = XGBClassifier(eval_metric=\"mlogloss\")\n",
    "\n",
    "# Define the SMOTE pipeline\n",
    "# smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "model = imPipeline([\n",
    "    # (\"smote\", smote),  # Apply SMOTE to balance the classes\n",
    "    (\"classifier\", xgb_model)  # XGBoost classifier\n",
    "])\n",
    "\n",
    "# Step 5: Train the model\n",
    "model.fit(X_train_combined, y_train_encoded)\n",
    "\n",
    "# Step 6: Predictions and evaluation\n",
    "y_pred = model.predict(X_test_combined)\n",
    "print(classification_report(y_test_encoded, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parker/.local/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "/home/parker/.local/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [12:04:01] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7758\n"
     ]
    }
   ],
   "source": [
    "bert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Split data into features and target\n",
    "X = df[[\"summary\", \"description\", \"requestType\", \"source\", \"causeOfIssue\"]]\n",
    "y = df[\"initial_category\"]\n",
    "\n",
    "# Encode the target labels as integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Process the categorical variables using OneHotEncoder\n",
    "categorical_columns = [\"requestType\", \"source\", \"causeOfIssue\"]\n",
    "encoder = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n",
    "\n",
    "X_train_cat = encoder.fit_transform(X_train[categorical_columns])\n",
    "X_test_cat = encoder.transform(X_test[categorical_columns])\n",
    "\n",
    "# Combine the 'summary' and 'description' columns to create text embeddings\n",
    "X_train_text = [summary + \" \" + description for summary, description in zip(X_train[\"summary\"], X_train[\"description\"])]\n",
    "X_test_text = [summary + \" \" + description for summary, description in zip(X_test[\"summary\"], X_test[\"description\"])]\n",
    "\n",
    "# Generate the text embeddings using the pre-trained BERT model\n",
    "X_train_embeddings = bert_model.encode(X_train_text)\n",
    "X_test_embeddings = bert_model.encode(X_test_text)\n",
    "\n",
    "# Combine the text embeddings with the one-hot encoded categorical features\n",
    "X_train_combined = np.hstack([X_train_embeddings, X_train_cat])\n",
    "X_test_combined = np.hstack([X_test_embeddings, X_test_cat])\n",
    "\n",
    "# Train the XGBoost model\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\")\n",
    "xgb_model.fit(X_train_combined, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = xgb_model.predict(X_test_combined)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "                               precision    recall  f1-score   support\n",
      "\n",
      "        Agency Administration       0.40      0.33      0.36        12\n",
      "             Billing / Grants       0.80      0.80      0.80         5\n",
      "            Delivery / Pickup       0.78      0.58      0.67        24\n",
      "Feedback - Concern / Negative       0.00      0.00      0.00         4\n",
      "          Feedback - Positive       0.67      0.40      0.50        10\n",
      "                         Menu       0.81      0.96      0.88       100\n",
      "        Orders - Pre delivery       0.80      0.80      0.80         5\n",
      "        Partner Produce Limit       1.00      0.50      0.67         4\n",
      "                  PartnerLink       0.00      0.00      0.00         1\n",
      "\n",
      "                     accuracy                           0.78       165\n",
      "                    macro avg       0.58      0.49      0.52       165\n",
      "                 weighted avg       0.75      0.78      0.75       165\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 4  0  1  0  0  7  0  0  0]\n",
      " [ 0  4  1  0  0  0  0  0  0]\n",
      " [ 0  1 14  1  0  8  0  0  0]\n",
      " [ 0  0  1  0  2  1  0  0  0]\n",
      " [ 0  0  1  0  4  5  0  0  0]\n",
      " [ 3  0  0  0  0 96  1  0  0]\n",
      " [ 0  0  0  0  0  1  4  0  0]\n",
      " [ 2  0  0  0  0  0  0  2  0]\n",
      " [ 1  0  0  0  0  0  0  0  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parker/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2326: UserWarning: labels size, 9, does not match size of target_names, 11\n",
      "  warnings.warn(\n",
      "/home/parker/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/parker/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/parker/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_, labels=np.unique(y_test)))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The number of FixedLocator locations (9), usually from a call to set_ticks, does not match the number of labels (11).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Plot confusion matrix using ConfusionMatrixDisplay\u001b[39;00m\n\u001b[1;32m      9\u001b[0m cm_display \u001b[38;5;241m=\u001b[39m ConfusionMatrixDisplay(confusion_matrix\u001b[38;5;241m=\u001b[39mcm, display_labels\u001b[38;5;241m=\u001b[39mlabel_encoder\u001b[38;5;241m.\u001b[39mclasses_)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mcm_display\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBlues\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Optional: Choose a color map for better visibility\u001b[39;00m\n\u001b[1;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/_plot/confusion_matrix.py:181\u001b[0m, in \u001b[0;36mConfusionMatrixDisplay.plot\u001b[0;34m(self, include_values, cmap, xticks_rotation, values_format, ax, colorbar, im_kw, text_kw)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m colorbar:\n\u001b[1;32m    180\u001b[0m     fig\u001b[38;5;241m.\u001b[39mcolorbar(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim_, ax\u001b[38;5;241m=\u001b[39max)\n\u001b[0;32m--> 181\u001b[0m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxticks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43myticks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxticklabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43myticklabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mylabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrue label\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPredicted label\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_ylim((n_classes \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m))\n\u001b[1;32m    191\u001b[0m plt\u001b[38;5;241m.\u001b[39msetp(ax\u001b[38;5;241m.\u001b[39mget_xticklabels(), rotation\u001b[38;5;241m=\u001b[39mxticks_rotation)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/artist.py:147\u001b[0m, in \u001b[0;36mArtist.__init_subclass__.<locals>.<lambda>\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_autogenerated_signature\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# Don't overwrite cls.set if the subclass or one of its parents\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# has defined a set method set itself.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m# If there was no explicit definition, cls.set is inherited from\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# the hierarchy of auto-generated set methods, which hold the\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# flag _autogenerated_signature.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[43mArtist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/artist.py:1231\u001b[0m, in \u001b[0;36mArtist.set\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1228\u001b[0m     \u001b[38;5;66;03m# docstring and signature are auto-generated via\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m     \u001b[38;5;66;03m# Artist._update_set_signature_and_docstring() at the end of the\u001b[39;00m\n\u001b[1;32m   1230\u001b[0m     \u001b[38;5;66;03m# module.\u001b[39;00m\n\u001b[0;32m-> 1231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/artist.py:1223\u001b[0m, in \u001b[0;36mArtist._internal_update\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_internal_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, kwargs):\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;124;03m    Update artist properties without prenormalizing them, but generating\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;124;03m    errors as if calling `set`.\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m \n\u001b[1;32m   1221\u001b[0m \u001b[38;5;124;03m    The lack of prenormalization is to maintain backcompatibility.\u001b[39;00m\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_props\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{cls.__name__}\u001b[39;49;00m\u001b[38;5;124;43m.set() got an unexpected keyword argument \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{prop_name!r}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/artist.py:1199\u001b[0m, in \u001b[0;36mArtist._update_props\u001b[0;34m(self, props, errfmt)\u001b[0m\n\u001b[1;32m   1196\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(func):\n\u001b[1;32m   1197\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1198\u001b[0m                     errfmt\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m), prop_name\u001b[38;5;241m=\u001b[39mk))\n\u001b[0;32m-> 1199\u001b[0m             ret\u001b[38;5;241m.\u001b[39mappend(\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret:\n\u001b[1;32m   1201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpchanged()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/axes/_base.py:74\u001b[0m, in \u001b[0;36m_axis_method_wrapper.__set_name__.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/_api/deprecation.py:297\u001b[0m, in \u001b[0;36mrename_parameter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m     warn_deprecated(\n\u001b[1;32m    293\u001b[0m         since, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mold\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas been renamed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m since Matplotlib \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msince\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; support \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor the old name will be dropped %(removal)s.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    296\u001b[0m     kwargs[new] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(old)\n\u001b[0;32m--> 297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/axis.py:1969\u001b[0m, in \u001b[0;36mAxis.set_ticklabels\u001b[0;34m(self, labels, minor, fontdict, **kwargs)\u001b[0m\n\u001b[1;32m   1965\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(locator, mticker\u001b[38;5;241m.\u001b[39mFixedLocator):\n\u001b[1;32m   1966\u001b[0m     \u001b[38;5;66;03m# Passing [] as a list of labels is often used as a way to\u001b[39;00m\n\u001b[1;32m   1967\u001b[0m     \u001b[38;5;66;03m# remove all tick labels, so only error for > 0 labels\u001b[39;00m\n\u001b[1;32m   1968\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(locator\u001b[38;5;241m.\u001b[39mlocs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(labels) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1969\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1970\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of FixedLocator locations\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1971\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(locator\u001b[38;5;241m.\u001b[39mlocs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m), usually from a call to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1972\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m set_ticks, does not match\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1973\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m the number of labels (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(labels)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1974\u001b[0m     tickd \u001b[38;5;241m=\u001b[39m {loc: lab \u001b[38;5;28;01mfor\u001b[39;00m loc, lab \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(locator\u001b[38;5;241m.\u001b[39mlocs, labels)}\n\u001b[1;32m   1975\u001b[0m     func \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_with_dict, tickd)\n",
      "\u001b[0;31mValueError\u001b[0m: The number of FixedLocator locations (9), usually from a call to set_ticks, does not match the number of labels (11)."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix using ConfusionMatrixDisplay\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\n",
    "cm_display.plot(cmap='Blues')  # Optional: Choose a color map for better visibility\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
